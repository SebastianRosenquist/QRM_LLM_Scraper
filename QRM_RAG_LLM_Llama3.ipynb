{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "For this Quantative Risk Assesment (QRM) assignement, we will be comparing and quantativly assesing the usability of a Large Language Model (LLM) - in this case Llama3 by Meta - when implemented in a company use case.\n",
    "\n",
    "### The Case\n",
    "The project case focuses on evaluating the risks and benefits of using LLMs to enhance web-scrapers/AI-powered asistants, specifically by comparing their performance with and without Retrieval-Augmented Generation (RAG). The goal is to quantify how RAG reduces risks such as hallucination and data inaccuracies, using metrics like Error Rate (ER), and to assess the trade-offs in implementing this approach in a quantitative risk management framework.\n",
    "\n",
    "### The Model\n",
    "Llama 3 by Meta is a transformer-based Large Language Model designed for natural language understanding and generation. It has a data cut-off point from mid-2023, meaning it is trained on datasets available up until that period. Llama 3 is optimized for high efficiency, offering a balance between computational demands and performance, and it supports integration with external tools such as Retrieval-Augmented Generation (RAG) to enhance its ability to work with up-to-date or domain-specific information beyond its training data.\n",
    "\n",
    "### The Data\n",
    "For our data, we will use the AI-act from the EU, published in 2024. The EU AI Act dataset is particularly suitable for this project because it was published after Llama 3’s data cut-off point (mid-2023). This ensures that the model has not been trained on this document, providing an ideal scenario to evaluate the model's ability to interpret and summarize new, unseen data. \n",
    "\n",
    "Furthermore, the EU AI Act contains complex and structured information, making it a robust test for assessing the model’s performance in understanding and accurately representing legal and technical content. This setup allows for testing how Retrieval-Augmented Generation (RAG) can improve accuracy and reduce hallucinations when integrated. \n",
    "\n",
    "For the LLM to understand the information, the data will be indexed into a vector database to make it searchable by the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model, tokenizer, query pipeline\n",
    "\n",
    "Define the model, the device, and the `bitsandbytes` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# using the `bitsandbytes` library\n",
    "# we set quantization configuration to load large model with less GPU memory\n",
    "# possibly not needed with larger setups\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the tokenizer and the model. To gain a sense of how long these steps will take, we wrap the pipeline in a timer. \n",
    "\n",
    "Be aware that running this model without a dedicated GPU won't be possible. To run this project I've used UCloud computers with  that were allocated to me for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "   model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We next define the query pipeline.\n",
    "\n",
    "To ensure proper functionality when setting up the HuggingFace pipeline, we especially need to specify the `max_length` here. This prevents the pipeline from defaulting to the very short length of 20 which we can't use for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        max_length=1000,\n",
    "        device_map=\"auto\",)\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now test our pipeline by defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, message):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        message: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"    \n",
    "    time_start = time()\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        message,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    \n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(message)]\n",
    "    answer = sequences[0]['generated_text'][len(message):]\n",
    "    \n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our pipeline and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the pipeline using a few queries related to the European Union Artificial Intelligence Act (EU AI Act).\n",
    "\n",
    "Additionally, we create a UI-utility function to neatly display the LLM's output. This function includes the calculation time, the input question, and the generated answer, all formatted for clear and easy readability. This will help us later when we aim to compare the output between the model with and without RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Let's test the model with a few questions regarding the AI-act. \n",
    "\n",
    "Later we will build a repo of all questions and save the responses in seperate columns for RAG and non-RAG responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will ask `What is the purpose of the EU AI regulation act?`. Here we are looking for the following from page 1 of the act: \n",
    "\n",
    "<I>\"The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal \n",
    "framework in particular for the development, the placing on the market, the putting into service and the use of \n",
    "artificial intelligence systems (AI systems) in the Union, in accordance with Union values, to promote the uptake of \n",
    "human centric and trustworthy artificial intelligence (AI) while ensuring a high level of protection of health, safety, \n",
    "fundamental  rights  as  enshrined  in  the  Charter  of  Fundamental  Rights  of  the  European  Union  (the  ‘Charter’), \n",
    "including democracy, the rule of  law and environmental protection, to protect against the harmful effects of AI \n",
    "systems  in  the  Union,  and  to  support  innovation.  This  Regulation  ensures  the  free  movement,  cross-border,  of \n",
    "AI-based  goods  and  services,  thus  preventing  Member  States  from  imposing  restrictions  on  the  development, \n",
    "marketing and use of AI systems, unless explicitly authorised by this Regulation.\"</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"What is the purpose of the EU AI regulation act?\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will ask `How many recitals are included in the first section of the EU AI Act document? Please provide the last numbered recital for an exact count`. \n",
    "\n",
    "This anwser is quite straightforward, since there are 180 of these recitals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"How many recitals are included in the first section of the EU AI Act document? Please provide the last numbered recital for an exact count\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the answers are not really helpfull or accurate. The model is obviously halucinating since we are asking for information that it has not been trained on or \"seen\" before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG)\n",
    "\n",
    "To improve the accuracy of our model, we should build a RAG system that will allow the model to \"read\" our document before responding.\n",
    "To create this RAG system we will be using a HuggingFacePipeline. The overall stepes are as follows:\n",
    "- Using a `HuggingFacePipline`, we will test the model\n",
    "- We will then ingest the EU AI act using `PyPdfLoader`\n",
    "- Sperate or Chunk the AI act into chuncks of 1000 characters, making sure that we have a partial overlap between the chunks of 100 characters.\n",
    "- Generate embeddings and store the transformed text (processed from the PDF, chunked with overlap, embedded, and indexed) into the vector database.\n",
    "- Build a `QA_Request` pipeline, incorporating both the retrieval and generation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model with HuggingFace Pipeline\n",
    "\n",
    "We will test the model using a HuggingFace pipeline by querying about the meaning of the EU AI Act. By utilizing the HuggingFacePipeline, we will have a more seamless integration with LangChain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "time_start = time()\n",
    "question = \"Please explain what EU AI Act is.\"\n",
    "response = llm(prompt=question)\n",
    "time_end = time()\n",
    "total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "full_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "display(Markdown(colorize_text(full_response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
