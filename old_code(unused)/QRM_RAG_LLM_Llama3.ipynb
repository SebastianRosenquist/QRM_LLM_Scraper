{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "For this Quantative Risk Assesment (QRM) assignement, we will be comparing and quantativly assesing the usability of a Large Language Model (LLM) - in this case Llama3 by Meta - when implemented in a company use case.\n",
    "\n",
    "### The Case\n",
    "The project case focuses on evaluating the risks and benefits of using LLMs to enhance web-scrapers/AI-powered asistants, specifically by comparing their performance with and without Retrieval-Augmented Generation (RAG). The goal is to quantify how RAG reduces risks such as hallucination and data inaccuracies, using metrics like Error Rate (ER), and to assess the trade-offs in implementing this approach in a quantitative risk management framework.\n",
    "\n",
    "### The Model\n",
    "Llama 3 by Meta is a transformer-based Large Language Model designed for natural language understanding and generation. It has a data cut-off point from mid-2023, meaning it is trained on datasets available up until that period. Llama 3 is optimized for high efficiency, offering a balance between computational demands and performance, and it supports integration with external tools such as Retrieval-Augmented Generation (RAG) to enhance its ability to work with up-to-date or domain-specific information beyond its training data.\n",
    "\n",
    "### The Data\n",
    "For our data, we will use the AI-act from the EU, published in 2024. The EU AI Act dataset is particularly suitable for this project because it was published after Llama 3’s data cut-off point (mid-2023). This ensures that the model has not been trained on this document, providing an ideal scenario to evaluate the model's ability to interpret and summarize new, unseen data. \n",
    "\n",
    "Furthermore, the EU AI Act contains complex and structured information, making it a robust test for assessing the model’s performance in understanding and accurately representing legal and technical content. This setup allows for testing how Retrieval-Augmented Generation (RAG) can improve accuracy and reduce hallucinations when integrated. \n",
    "\n",
    "For the LLM to understand the information, the data will be indexed into a vector database to make it searchable by the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model, tokenizer, query pipeline\n",
    "\n",
    "Define the model, the device, and the `bitsandbytes` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# using the `bitsandbytes` library\n",
    "# we set quantization configuration to load large model with less GPU memory\n",
    "# possibly not needed with larger setups\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the tokenizer and the model. To gain a sense of how long these steps will take, we wrap the pipeline in a timer. \n",
    "\n",
    "Be aware that running this model without a dedicated GPU won't be possible. To run this project I've used UCloud computers with  that were allocated to me for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "   model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We next define the query pipeline.\n",
    "\n",
    "To ensure proper functionality when setting up the HuggingFace pipeline, we especially need to specify the `max_length` here. This prevents the pipeline from defaulting to the very short length of 20 which we can't use for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        max_length=1000,\n",
    "        device_map=\"auto\",)\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now test our pipeline by defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, message):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        message: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"    \n",
    "    time_start = time()\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        message,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    \n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(message)]\n",
    "    answer = sequences[0]['generated_text'][len(message):]\n",
    "    \n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our pipeline and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the pipeline using a few queries related to the European Union Artificial Intelligence Act (EU AI Act).\n",
    "\n",
    "Additionally, we create a UI-utility function to neatly display the LLM's output. This function includes the calculation time, the input question, and the generated answer, all formatted for clear and easy readability. This will help us later when we aim to compare the output between the model with and without RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Let's test the model with a few questions regarding the AI-act. \n",
    "\n",
    "Later we will build a repo of all questions and save the responses in seperate columns for RAG and non-RAG responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will ask `What is the purpose of the EU AI regulation act?`. Here we are looking for the following from page 1 of the act: \n",
    "\n",
    "<I>\"The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal \n",
    "framework in particular for the development, the placing on the market, the putting into service and the use of \n",
    "artificial intelligence systems (AI systems) in the Union, in accordance with Union values, to promote the uptake of \n",
    "human centric and trustworthy artificial intelligence (AI) while ensuring a high level of protection of health, safety, \n",
    "fundamental  rights  as  enshrined  in  the  Charter  of  Fundamental  Rights  of  the  European  Union  (the  ‘Charter’), \n",
    "including democracy, the rule of  law and environmental protection, to protect against the harmful effects of AI \n",
    "systems  in  the  Union,  and  to  support  innovation.  This  Regulation  ensures  the  free  movement,  cross-border,  of \n",
    "AI-based  goods  and  services,  thus  preventing  Member  States  from  imposing  restrictions  on  the  development, \n",
    "marketing and use of AI systems, unless explicitly authorised by this Regulation.\"</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"What is the purpose of the EU AI regulation act?\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will ask `How many recitals are included in the first section of the EU AI Act document? Please provide the last numbered recital for an exact count`. \n",
    "\n",
    "This anwser is quite straightforward, since there are 180 of these recitals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"How many recitals are included in the first section of the EU AI Act document? Please provide the last numbered recital for an exact count\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the answers are not really helpfull or accurate. The model is obviously halucinating since we are asking for information that it has not been trained on or \"seen\" before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG)\n",
    "\n",
    "To improve the accuracy of our model, we should build a RAG system that will allow the model to \"read\" our document before responding.\n",
    "To create this RAG system we will be using a HuggingFacePipeline. The overall stepes are as follows:\n",
    "- Using a `HuggingFacePipline`, we will test the model\n",
    "- We will then ingest the EU AI act using `PyPdfLoader`\n",
    "- Sperate or Chunk the AI act into chuncks of 1000 characters, making sure that we have a partial overlap between the chunks of 100 characters.\n",
    "- Generate embeddings and store the transformed text (processed from the PDF, chunked with overlap, embedded, and indexed) into the vector database.\n",
    "- Build a `QA_Request` pipeline, incorporating both the retrieval and generation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model with HuggingFace Pipeline\n",
    "\n",
    "We will test the model using a HuggingFace pipeline by querying about the meaning of the EU AI Act. By utilizing the HuggingFacePipeline, we will have a more seamless integration with LangChain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "time_start = time()\n",
    "question = \"Please explain what EU AI Act is.\"\n",
    "response = llm(prompt=question)\n",
    "time_end = time()\n",
    "total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "full_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "display(Markdown(colorize_text(full_response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now ingest our data (EU AI Act). We will be using the `PyPDFLoader` from Langchain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./EU_AI_Act.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Using a recursive character text splitter, we will split our data into chunks.\n",
    "- chunk_size: 1000 (the size of a chunk in characters)\n",
    "- chunk_overlap: 100 (the size of characters that two chunks overlap)\n",
    "\n",
    "Chunk overlapping balances chunk size constraints and context preservation, enhancing the effectiveness and accuracy of LLMs in processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings with Sentence Transformers and HuggingFace\n",
    "\n",
    "We generate embeddings using **Sentence Transformer** models and **HuggingFace embeddings**. \n",
    "\n",
    "#### Handling Availability Issues\n",
    "Occasionally, HuggingFace's sentence-transformer models may not be accessible online. To address this, we implement a mechanism that enables the use of locally stored Sentence Transformer models, ensuring uninterrupted functionality even in offline or restricted environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "# try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2\n",
    "try:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    # alternatively, we will access the embeddings models locally\n",
    "    local_model_path = \"/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing ChromaDB with Document Splits and Embeddings\n",
    "\n",
    "We initialize **ChromaDB** using the following:\n",
    "\n",
    "- **Document Splits:** Pre-processed chunks of the original text.\n",
    "- **Embeddings:** Generated embeddings from the previously defined models.\n",
    "\n",
    "#### Enabling Persistence\n",
    "To ensure data is stored and accessible for future use, we enable the **persistence** option for the vector database, allowing ChromaDB to save data locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Chain\n",
    "\n",
    "We initialize a **QA_Retrival task chain** using LangChain utilities.\n",
    "\n",
    "#### How It Works\n",
    "1. **Querying the Vector Database:**  \n",
    "   The chain begins by querying the vector database using **similarity search** with the provided prompt.\n",
    "\n",
    "2. **Retrieving Context:**  \n",
    "   The vector database retrieves relevant documents that match the query.\n",
    "\n",
    "3. **Composing the Prompt:**  \n",
    "   The query and the retrieved context are combined to create a prompt. This prompt instructs the LLM to answer the query.\n",
    "\n",
    "4. **Generating the Response:**  \n",
    "   The LLM uses the retrieved context to generate an accurate and context-aware response.\n",
    "\n",
    "This process is known as **Retrieval-Augmented Generation (RAG)** because it combines **retrieval** of relevant data with **generation** of responses based on that data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the RAG\n",
    "\n",
    "We now define a new test function that will run our query and provide us with a RAG-based response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(qa, query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval Augmented Generation (RAG) system.\n",
    "    \n",
    "    Args:\n",
    "        qa (RetrievalQA.from_chain_type): Langchain function to perform RAG\n",
    "        query (str): query for the RAG system\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    time_start = time()\n",
    "    response = qa.run(query)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "\n",
    "    full_response =  f\"Question: {query}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "    display(Markdown(colorize_text(full_response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part - checking our queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is performed the testing of high-risk AI systems in real world conditions?\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the operational obligations of notified bodies?\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Sources\n",
    "\n",
    "To check the document sources for the last query, we follow these steps:\n",
    "\n",
    "1. **Run a Similarity Search:**  \n",
    "   Perform a similarity search in the vector database based on the query.\n",
    "\n",
    "2. **Iterate Through Retrieved Documents:**  \n",
    "   Loop through the documents returned by the similarity search.\n",
    "\n",
    "3. **Extract and Display Metadata:**  \n",
    "   For each document:\n",
    "   - Print the document source from its metadata.\n",
    "   - Display the page content of the document.\n",
    "\n",
    "This process helps identify the origin and content of the documents retrieved during the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "We utilized **LangChain**, **ChromaDB**, and **Llama3** as the LLM to build a **Retrieval-Augmented Generation (RAG)** solution. For testing purposes, we worked with the **EU AI Act (2023)**.\n",
    "\n",
    "#### Key Findings\n",
    "- The RAG model successfully provided accurate answers to questions about the EU AI Act.\n",
    "\n",
    "#### Areas for Improvement\n",
    "To enhance the solution, we plan to:\n",
    "1. **Optimize the Embeddings:** Improve the quality and relevance of the embeddings used in the retrieval process.\n",
    "2. **Implement Advanced RAG Schemes:** Explore and apply more complex RAG architectures for better performance and flexibility.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
