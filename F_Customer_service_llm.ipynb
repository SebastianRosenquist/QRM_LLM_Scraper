{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img img class=\"logo\" src=\"https://www.cbs.dk/files/cbs.dk/cbslogo.png\" style=\"width: 300px;\" align=\"right\">\n",
    "</div>\n",
    "\n",
    "# Exercise 05\n",
    "\n",
    "# Customer Service LLM Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Phi3 LLM from Miscrosoft: [llm](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "\n",
    "LLM governance refers to the frameworks and policies guiding the development, deployment, and oversight of large language models. It addresses ethical, legal, and societal challenges, ensuring that these powerful tools are used responsibly and for the public good. Effective governance balances innovation with safeguards against misuse, bias, and unintended consequences. As LLMs continue to evolve, robust governance is crucial for maintaining trust and accountability in their use across diverse sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get your environment setup and rweady so you can deploy a small language model with Gen AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be worth checking this out if I have a GPU: https://github.com/microsoft/onnxruntime-genai/blob/main/examples/python/phi-3-tutorial.md#run-with-directml\n",
    "\n",
    "Also useful: https://www.kaggle.com/code/gpreda/rag-using-llama3-langchain-and-chromadb/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seb_R\\HA(IT) - Programmering Projekter\\QRM_LLM_Scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]Downloading 'cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.694e8a697352ea15cead99bfbc680f7237eacad08baddfc07d60c8d00f49cd43.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data.27bf3bfa654e50b3b734948f7be1831c2c91787ec80ae66b497fd30c2ddff443.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/genai_config.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\genai_config.json.eaf8dc9f3d49cdf78273dfead27ce250e3eb5509.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/tokenizer.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\tokenizer.json.efc309ef56b8d8fba1b50d1b4a6e5be6cfded459.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/config.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\config.json.01de35834a12bca5fc9150cc2a8351135f442757.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/special_tokens_map.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\special_tokens_map.json.32b360b36e8255e8346f50942f478e5a2227e2e6.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/configuration_phi3.py' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\configuration_phi3.py.f4553db23ac65c608fd150a14acbd04d3ff80a0f.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/added_tokens.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\added_tokens.json.4dece7ae8bbeb8f468cb1da428bfb6193ae0751c.incomplete'\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\genai_config.json\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\special_tokens_map.json\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\configuration_phi3.py\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\config.json\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\added_tokens.json\n",
      "\n",
      "Fetching 10 files:  10%|█         | 1/10 [00:00<00:03,  2.51it/s]Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx\n",
      "\n",
      "Fetching 10 files:  50%|█████     | 5/10 [00:00<00:00, 11.76it/s]Downloading 'cuda/cuda-int4-rtn-block-32/tokenizer_config.json' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\tokenizer_config.json.9d9d37222d0f5ad9b2f02408b13ec21b8023a93f.incomplete'\n",
      "Downloading 'cuda/cuda-int4-rtn-block-32/tokenizer.model' to '.cache\\huggingface\\download\\cuda\\cuda-int4-rtn-block-32\\tokenizer.model.9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347.incomplete'\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\tokenizer_config.json\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\tokenizer.model\n",
      "Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\tokenizer.json\n",
      "\n",
      "Fetching 10 files:  50%|█████     | 5/10 [00:20<00:00, 11.76it/s]Download complete. Moving file to cuda\\cuda-int4-rtn-block-32\\phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data\n",
      "\n",
      "Fetching 10 files:  60%|██████    | 6/10 [01:30<01:23, 20.93s/it]\n",
      "Fetching 10 files: 100%|██████████| 10/10 [01:30<00:00,  9.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Prepend \\\\?\\ to the absolute path\n",
    "local_dir = f\"\\\\\\\\?\\\\{os.path.abspath(cwd)}\"\n",
    "\n",
    "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cuda/cuda-int4-rtn-block-32/* --local-dir .\n",
    "\n",
    "#!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include directml/* --local-dir .\n",
    "\n",
    "# Run the command with the modified path\n",
    "#!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir \"{local_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  4427  100  4427    0     0  21882      0 --:--:-- --:--:-- --:--:-- 22135\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install --pre onnxruntime-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snibbet will run the cloned python script and let you start the llm. Now you have a LLM running on your local machine. When cloding the Notebook or shell you will stop it and the memory will be free again for other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you have your environment set up and you have an llm running which you can use. You can also access it via the comand line by opening the shell in Jupiter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 05.1\n",
    "\n",
    "Follow the task mentioned above and:\n",
    "* Run your local LLM\n",
    "* Test it with some prompts\n",
    "* Note doen youyr observations\n",
    "* Compare it with your experience with ChatGPT\n",
    "* Fill out the reporting questions on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
